[
["introduction.html", "Soil Predictions using MIR-Spectroscopy 1 Introduction", " Soil Predictions using MIR-Spectroscopy 1 Introduction This guide documents the use of MIR-spectroscopy at the Woods Hole Research Center, to predict various soil health properties. It is aimed at helping other teams build soil spectral libraries, create models from their data, access model performance against known values, and ultimately make predictions on new data. The machine learning methods outlined in this document were selected after assessing the performance of a variety of models. This process is explained in the following paper: Dangal S.R.S., J. Sanderman, S. Wills, and L. Rameriz-Lopez. 2019. Accurate and Precise Prediction of Soil Properties from a Large Mid-Infrared Spectral Library. Soil Systems 3(1):11. doi:10.3390/soilsystems3010011 Questions and comments can be sent to jsanderman@whrc.org or crivard@whrc.org Woods Hole Research Center Website: whrc.org Instagram: woodsholeresearchcenter Address: 149 Woods Hole Rd, Falmouth, MA 02540 "],
["background.html", "2 Background", " 2 Background Mid-Infrared (MIR) spectroscopy is a cost effective technique for predicting a variety of soil health properties. "],
["data-preprocessing.html", "3 Data Preprocessing 3.1 Extract OPUS files 3.2 Process Spectra 3.3 Merge with Lab Data 3.4 Select Calibration Set", " 3 Data Preprocessing 3.1 Extract OPUS files For Bruker Instruments, an OPUS file containing spectral data, will be output for each sample that is scanned. To compile these separate files into one dataset, we use a couple functions from the ‘simplerspec’ package by Philip Baumann, as well as the stringr and foreach packages. #---Packages---# library(stringr) #used for str_sub library(foreach) #used within read-opus-universal.R source(&quot;Single_Lib/reference_files/gather-spc.R&quot;) #simplerspec function source(&quot;Single_Lib/reference_files/read-opus-universal.R&quot;) #simplerspec function Gets the paths of all OPUS files… #---List Files---# spectraPath &lt;- &quot;/Single_Lib/SPECTRA&quot; #folder of OPUS files dirs &lt;- list.dirs(paste(getwd(),spectraPath,sep=&quot;&quot;), full.names=TRUE) all.files &lt;- list.files(dirs, pattern= &quot;*.0&quot;, recursive=TRUE,full.names=TRUE) Extracts the spectra and gathers it into a tibble data frame… #---Extract Spectra---# spc_list &lt;- read_opus_univ(fnames = all.files, extract = c(&quot;spc&quot;)) ## Extracted spectra data from file: &lt;WHRC03405_S_001_030.0&gt; ## Extracted spectra data from file: &lt;WHRC03406_S_002_014.0&gt; ## Extracted spectra data from file: &lt;WHRC03407_S_002_046.0&gt; soilspec_tbl &lt;- spc_list %&gt;% gather_spc() spc &lt;- soilspec_tbl$spc Optionally truncates the dataset to ensure the spectra from different samples align. Only necessary if instrument settings are changed between runs. 3017 would be changed to the number of spectral columns (wavelengths collected) spc &lt;- lapply(1:length(spc),function(x) spc[[x]][,1:3017]) Processes spectra into a dataframe and assigns a sample_id, based off the file names… *sample_ids that are numeric may cause issues while merging so a string ID is advised. spc.df &lt;- as.data.frame(matrix(unlist(spc), nrow=length(spc), byrow=T)) colnames(spc.df) &lt;- colnames(spc[[1]]) spc.df &lt;- data.frame(sample_id = soilspec_tbl$sample_id, spc.df) spc.df$sample_id &lt;- str_sub(spc.df$sample_id,1,9) Optionally saves the spectra as an R dataset or csv file… save(spc.df, file=&quot;spectra_original.RData&quot;) write.csv(spc.df, &quot;spectra_original.csv&quot;) 3.2 Process Spectra We narrow down the regions of the spectra by truncating wavenumbers below 628 and between 2268 to 2389, which is a CO2 sensitive region. #---Edit Spectral Columns---# col.names &lt;- colnames(spectra$spc) #get column names which are wavenumbers col.names &lt;- as.numeric(substring(col.names,2)) cutoff &lt;- which(col.names &lt;= 628)[1] spectra$spc &lt;- spectra$spc[,-c(cutoff:length(col.names))] #truncate at &gt;= 628 min.index &lt;- which(col.names &lt;= 2389)[1] max.index &lt;- which(col.names &lt;= 2268)[1] spectra$spc &lt;- spectra$spc[,-c(min.index:max.index)] #remove CO2 region We perform a baseline transformation to normalize the spectra #---Baseline Transformation---# library(matrixStats) base_offset &lt;- function(x){ test &lt;- rowMins(x) return(x-test) } spectra$spc &lt;- base_offset(spectra$spc) Optionally saves the processed spectra as an R dataset or csv file… #---Save Spectra---# save(spc.df, file=&quot;Single_Lib/spectra_processed.RData&quot;) write.csv(spc.df, &quot;Single_Lib/spectra_processed.csv&quot;) 3.3 Merge with Lab Data If there is lab data associated with your soil samples, this can be merged with the spectral data and later used to assess the performance of your models.The example lab dataset below provides information about where the soil sample was taken with the Site_ID and Horizon, as well as the lab measurements for various soil properties including Organic Carbon, Sand, Silt and Clay. #---Read Lab Data---# library(readr) #used to open the .csv file lab &lt;- data.frame(read_csv(&quot;Single_Lib/LAB_DATA.csv&quot;, col_types=cols())) #read in the lab data The merge() command joins the lab dataset to the spectral dataset. The all.y=TRUE parameter indicates that the final dataset will contain all the rows of spectra. This means that if some samples do not have lab data, they will be assigned a value of NA but the spectra will remain in the set. #---Merge Data---# all_data &lt;- merge(lab, spectra, all.y=TRUE) save(all_data, file=&quot;Single_Lib/spectra_lab_merge.RData&quot;) write.csv(all_data, &quot;Single_Lib/spectra_lab_merge.csv&quot;, row.names=FALSE) The final dataframe contains a unique ID, lab data, and a matrix of spectral data called ‘spc’. It is suggested to save this file as RData so it may be reloaded as needed. 3.4 Select Calibration Set Once the full dataset is processed, it must be split into calibration and validation sets to that will be used to train and test the model. This process must be repeated for each soil property that is being predicted, since outliers and NA values may vary among the properties. The example below shows this step for a single property, while the RUNFILE code shows it implemented in a loop for several properties. #---Packages---# library(pls) #used for plsr model source(&quot;Single_Lib/reference_files/functions_modelChoice.R&quot;) #used for optimum_sd_outlier() Rows that have NA or negative values for the property being predicted (% Organic Carbon) are excluded from the dataset. #---Eliminate NA &amp; negative---# all_data &lt;- all_data[!is.na(all_data[,property]),] #no NAs all_data &lt;- all_data[which(all_data[,property] &gt; 0),] #no negative values A pls model is created from the remaining data to flag outliers. Samples that have inaccurate predictions are excluded. This is measured by regressing the predictions against its associated lab data, and evaluating how many standard deviations each sample is from the best fit line. Those &gt;99th percentile are removed since… #---Remove Outliers---# pls.fit &lt;- plsr(sqrt(get(property))~spc, ncomp= 20, data = all_data, valid=&quot;CV&quot;, segments = 50) #fit a pls model to the data pred &lt;- c(predict(pls.fit, newdata = all_data$spc,ncomp=20))^2 #make predictions sd.outlier &lt;- optimum_sd_outlier(pred, all_data[,property], seq(0.1,3, by =0.02)) #flag samples that performed poorly as outliers row.index &lt;- outlier(pred, all_data[,property], sd.outlier[1]) if(length(row.index) &gt; 0){ all_data &lt;- all_data[row.index,] #subset non-outliers } Performing kennard stone to separate data into 80% calibration and 20% validation sets. This step can be skipped if using MBL modeling approach which uses the entire dataset. If both modeling approaches are being used, you can load and row bind the calibration and validation sets for MBL. #---Kennard Stone---# ken_stone&lt;- prospectr::kenStone(X = all_data$spc, k = as.integer(0.8*nrow(all_data)), metric = &quot;mahal&quot;, pc = 10) calib &lt;- all_data[ken_stone$model, ] #subset calibration set valid &lt;- all_data[ken_stone$test, ] #subset validation set Save calibration and validation set for that particular property #Save for property save(calib, file=paste(&quot;Single_Lib/calib&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;)) save(valid, file=paste(&quot;Single_Lib/valid&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;)) An alternative to saving the datasets would be to add a column indicating whether a sample falls under the calibration or validation set. This could be used to subset the correct datasets when making predictions. If the full dataset is large and takes a while to load, saving the former approach is preferred. If the set loads relatively quickly, it may be worth it to save storage space and use the latter method. "],
["plsr-models.html", "4 PLSR Models 4.1 Model Theory 4.2 Making PLSR Predictions", " 4 PLSR Models This section walks through the code for creating a parial least squares regression from your data and using it to make predictions. 4.1 Model Theory Partial Least Squares Regression (PLSR) is a useful technique for making predictions on high dimensional datasets; Those with many columns or predictor variables relative to the number of rows or instances. In this example, we are using 2720 columns of spectral data as predictor variables for only 333 samples. A simple regression model would {not be good for these reasons}. PLSR models, like Principal Component Analysis (PCA), reduce the dimensionality of the dataset by creating new set of orthogonal variables that explain the most variation in the data. These come in the form of scores and loadings {explain contrast with PCA} vocab: orthogonal, high dimensional, latent variables 4.2 Making PLSR Predictions To predict using PLSR models, we use the pls package in r #---Packages---# library(pls) Assuming you have exited the environment where you pre-processed spectra, reload your calibration and validation sets. #---Load Data---# load(&quot;Single_Lib/calib.OC.RData&quot;) load(&quot;Single_Lib/valid.OC.RData&quot;) The plsr command creates a model based on the following inputs: Y The lab data/ observed data for the soil property you are trying to predict. We chose to square root transform this variable to normalize the data. Predictions made my the model are squared to back transform them. X A matrix of spectra with the same number of rows as Y ncomp The number of components that you would like to include in the model data The dataset containing Y and X valid The preferred validation type (“LOO”,“CV”,“none”) #---Create Model---# plsr.model &lt;- plsr(sqrt(get(property))~spc, ncomp=20, data = calib, valid=&quot;LOO&quot;) save(plsr.model, file = paste(&quot;plsr&quot;, property,&quot;.RData&quot;, sep=&quot;&quot;)) #saving the model Explain ncomp one sigma, how predictions are stored in plsr.model, do predictions on both calibration and validation data. Flag which are cal and val for each property. define valid in this file as where OC.cal==1. (Saves storage space, compromise is possibly runtime of getting that subset? But it seems like its quicker than loading all the data again) #---Applying Model---# ncomp.onesigma &lt;- selectNcomp(plsr.model, method = &quot;onesigma&quot;, plot = TRUE, ylim = c(0, 50)) predVals &lt;- c(predict(plsr.model, newdata = predDat$spc, ncomp=ncomp.onesigma))^2 savename &lt;- paste(property, modelType, predDatName, paste(&quot;v&quot;,datname,sep=&quot;&quot;), sep=&quot;.&quot;) "],
["mbl-models.html", "5 MBL Models 5.1 Model Theory 5.2 Making MBL Predictions", " 5 MBL Models Techniques used to impliment a memory-based learner are outlined in this section 5.1 Model Theory {Explanation of how mbl works with some pretty pictures, explain the package used} 5.2 Making MBL Predictions Xu &lt;- predDat$spc Yu &lt;- sqrt(predDat[,property]) Yr &lt;- sqrt(calib[,property]) Xr &lt;- calib$spc Xu &lt;- Xu[!is.na(Yu),] Yu &lt;- Yu[!is.na(Yu)] Xr &lt;- Xr[!is.na(Yr),] Yr &lt;- Yr[!is.na(Yr)] ctrl &lt;- mblControl(sm = &#39;pc&#39;, pcSelection = list(&#39;opc&#39;, 50), valMethod = &#39;loc_crossval&#39;,center=TRUE,scale=FALSE,allowParallel=FALSE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Yu = Yu, Xu = Xu, mblCtrl = ctrl, dissUsage = &#39;none&#39;, #k = seq(40, 100, by = 20), k = seq(10, 20, by = 2), method = &#39;pls&#39;, pls.c = 6) #save(mbl.sqrt, file= paste(savepath, savename,&quot;.RData&quot;, sep=&quot;&quot;)) #2- Applying the model #savepath &lt;- paste(&quot;./Models/USGS/Geochem/Predictions/&quot;, sep=&quot;&quot;) #savename &lt;- paste(property, modelType, fd, predDatName, paste(&quot;v&quot;,datname,sep=&quot;&quot;), sep=&quot;.&quot;) predVals &lt;- c(mbl.sqrt$results$Nearest_neighbours_40$pred)^2 #predVals &lt;- c(mbl.sqrt$results$Nearest_neighbours_14$pred)^2 } #{option 2-- weighted to closer neighbors} if(TRUE){ Xu &lt;- predDat$spc Yu &lt;- sqrt(predDat[,property]) Yr &lt;- sqrt(calib[,property]) Xr &lt;- calib$spc Xu &lt;- Xu[!is.na(Yu),] Yu &lt;- Yu[!is.na(Yu)] Xr &lt;- Xr[!is.na(Yr),] Yr &lt;- Yr[!is.na(Yr)] dmetric = &quot;pls&quot; diss2test &lt;- seq(0.3, 1, by=0.1) kminmax &lt;- c(10, nrow(calib$spc)) rmethod &lt;- &quot;wapls1&quot; pls.f &lt;- c(minpls=3, maxpls=20) ctrl &lt;- mblControl(sm = dmetric, pcSelection = list(&quot;opc&quot;, 50), valMethod = &quot;NNv&quot;, returnDiss = TRUE, scaled = FALSE, center = TRUE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Xu = Xu, mblCtrl = ctrl, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = rmethod) idx.best.ca &lt;- which.min(mbl.sqrt$nnValStats$st.rmse) best.kdiss.ca &lt;- mbl.sqrt$nnValStats$k.diss[idx.best.ca] ## Get the predicted values for the validation set predVals &lt;- c(getPredictions(mbl.sqrt)[, idx.best.ca])^2 } "],
["model-performance.html", "6 Model Performance 6.1 Statistics 6.2 Plots", " 6 Model Performance 6.1 Statistics Create summary table with the predictions against the lab data #---Summary Table---# col.names &lt;- colnames(predDat) propCol &lt;- which(col.names == toString(property))[1] pred_obs &lt;- data.frame(predDat[,1], predVals, (predDat[,propCol])*unit_adj) names(pred_obs) &lt;- c(&quot;ID&quot;, &quot;pred&quot;, &quot;obs&quot;) 6.2 Plots #---Validation Plot---# max &lt;- max(pred_obs[,c(&quot;pred&quot;, &quot;obs&quot;)]) plot.plsr(pred_obs$obs, pred_obs$pred, property, c(0,(1.1*max)),units) "],
["references.html", "References", " References "]
]
